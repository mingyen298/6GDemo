{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d656937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f88351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeFlow:\n",
    "    @staticmethod\n",
    "    def Compile(pipelineFunc , trainJobId):\n",
    "        print(os.path.dirname(os.path.realpath(__file__)))\n",
    "        def ZipDir(path,trainJobId):\n",
    "            zf = zipfile.ZipFile('{0}.zip'.format(trainJobId), 'w', zipfile.ZIP_DEFLATED)\n",
    "            for root, dirs, files in os.walk(path):\n",
    "                for file_name in files:\n",
    "                    zf.write(os.path.join(root, file_name))\n",
    "        os.mkdir(trainJobId)\n",
    "        func_content = inspect.getsource(pipelineFunc)\n",
    "        main_py_filename = \"{0}/main.py\".format(trainJobId)\n",
    "        with open(main_py_filename.format(trainJobId),\"w+\") as f:\n",
    "            f.write(func_content)\n",
    "            f.write(\"train_export_model()\\n\")\n",
    "            f.close()\n",
    "        container_sh_filename = \"{0}/container.sh\".format(trainJobId)\n",
    "        with open(container_sh_filename,\"w+\") as f:\n",
    "            # f.write(\"docker run -e JOB_ID={0} --name {0}_train_job --rm -v $HOME/Desktop/SideProject/6GDemo/FakeFlow/file/pipelines/{0}/main.py:/app/main.py train_env:1.1 python main.py\".format(trainJobId))\n",
    "            # f.write(\"docker run -e JOB_ID={0} --rm -v $PWD/file/pipelines/{0}/main.py:/app/main.py train_env:1.1 python main.py\".format(trainJobId))\n",
    "            f.write(\"docker run -e JOB_ID={0} --rm -v $PWD/file/pipelines/{0}/main.py:/app/main.py 6g-demo:train python main.py\".format(trainJobId))\n",
    "            f.close()\n",
    "        ZipDir(trainJobId,trainJobId)\n",
    "        shutil.rmtree(trainJobId, ignore_errors=True)\n",
    "        requests.post(\"http://172.17.0.1:3500/pipeline/upload/{0}\".format(trainJobId),files={'file':open(trainJobId+\".zip\", 'rb')})\n",
    "        os.remove(trainJobId+\".zip\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cf86883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_export_model():\n",
    "    import inspect\n",
    "    import requests\n",
    "    import numpy as np\n",
    "    from tensorflow.keras.models import Sequential, Model\n",
    "    from tensorflow.keras.layers import Dense, LSTM\n",
    "    import os\n",
    "    import zipfile\n",
    "    import paho.mqtt.client as mqtt\n",
    "    import json\n",
    "    TrainJobID = os.environ.get('JOB_ID')\n",
    "    def ZipDir(path,trainJobId):\n",
    "        zf = zipfile.ZipFile('{}.zip'.format(trainJobId), 'w', zipfile.ZIP_DEFLATED)\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for file_name in files:\n",
    "                zf.write(os.path.join(root, file_name))\n",
    "    class MQStatus:\n",
    "        def __init__(self):\n",
    "            self.topic = \"/mqtt\"\n",
    "            self.client = mqtt.Client()\n",
    "        def Publish(self,state):\n",
    "            dic = {\"uuid\":TrainJobID , \"state\":state}\n",
    "\n",
    "            message = json.dumps(dic)\n",
    "            self.client.publish(self.topic , message)\n",
    "        def Connect(self):\n",
    "            self.client.connect(\"yen-test.com\", 1883)\n",
    "    class ModelStorageSDK:\n",
    "        @staticmethod\n",
    "        def UploadModel(path,trainJobId):\n",
    "            ZipDir(path,trainJobId)\n",
    "            requests.post(\"http://172.17.0.1:3501/model/upload/{0}\".format(trainJobId),files={'file':open(\"{}.zip\".format(trainJobId), 'rb')})\n",
    "    class EasyLSTMModel:\n",
    "        def __init__(self,X,Y,sw_width,features,epochs_num):\n",
    "            self.X = X\n",
    "            self.Y = Y\n",
    "            self.sw_width = sw_width\n",
    "            self.features = features\n",
    "            self.epochs_num = epochs_num\n",
    "            self.model = None\n",
    "        def Training(self):\n",
    "            self.model = Sequential()\n",
    "            self.model.add(LSTM(50, activation='relu', input_shape=(self.sw_width, self.features)))\n",
    "            self.model.add(Dense(1))\n",
    "            self.model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "            self.model.summary()\n",
    "            history = self.model.fit(self.X, self.Y, epochs=self.epochs_num, verbose=0)\n",
    "            print('\\ntrain_acc:%s'%np.mean(history.history['accuracy']), '\\ntrain_loss:%s'%np.mean(history.history['loss']))\n",
    "\n",
    "        def Predict(self,testSeq):\n",
    "            print('yhat:%s'%(self.model.predict(testSeq)),'\\n-----------------------------')\n",
    "\n",
    "        def Save(self):\n",
    "            self.model.save(\"./model\")\n",
    "    class DataExtractor:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "        def getDBData(self,n):\n",
    "            rawDatas = []\n",
    "            for i in range(1,n+1):\n",
    "              rawDatas.append(i)\n",
    "            return rawDatas\n",
    "        def dataExtraction(self,rawDatas):\n",
    "            extractedDatas = []\n",
    "            cur = 0\n",
    "            length = len(rawDatas)\n",
    "            X,Y = [],[]\n",
    "            while cur < (length-3):\n",
    "              X.append(rawDatas[cur:cur+3])\n",
    "              Y.append(rawDatas[cur+3:cur+4])\n",
    "              cur = cur + 1\n",
    "            X = np.array(X)\n",
    "            Y = np.array(Y)\n",
    "            # print(\"X Shape:{}\",X.shape)\n",
    "            # print(X)\n",
    "            X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "            return X , Y\n",
    "\n",
    "        def Run(self):\n",
    "            rawDatas = self.getDBData(10)\n",
    "            return self.dataExtraction(rawDatas)\n",
    "    mq = MQStatus()\n",
    "    mq.Connect()\n",
    "    mq.Publish(1)\n",
    "    # 1. Data Extraction\n",
    "    extractor = DataExtractor()\n",
    "    # time.sleep(30)\n",
    "    X , Y = extractor.Run()\n",
    "    print(\"X Shape:{}\",X.shape)\n",
    "    print(X)\n",
    "    # 2. Model Training\n",
    "    # time.sleep(30)\n",
    "    mq.Publish(2)\n",
    "    model = EasyLSTMModel(X,Y,3,1,500)\n",
    "    model.Training()\n",
    "    # 3. Model Experiment\n",
    "    testDatas = np.array([5,6,7])\n",
    "    testDatas = testDatas.reshape((1, 3, 1))\n",
    "    print(\"testDatas Shape:{}\",testDatas.shape)\n",
    "    print(testDatas)\n",
    "    model.Predict(testDatas)\n",
    "    model.Save()\n",
    "    ModelStorageSDK.UploadModel(\"./model\",TrainJobID)\n",
    "    mq.Publish(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if len(sys.argv) > 1:\n",
    "    \n",
    "    trainJobId = sys.argv[1]\n",
    "    FakeFlow.Compile(train_export_model,trainJobId)\n",
    "    requests.post(\"http://172.17.0.1:3500/pipeline/start/{}\".format(trainJobId))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
